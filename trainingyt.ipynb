{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Training/playground\n",
    "\n",
    "In this tutorial, we briefly explain how to create and train a model using PyTorch. For inference, we'll show how to use the Intel AI software products to improve performance, apply quantization, or even use Intel GPU.\n",
    "\n",
    "In the beginning, we need to import some default modules and PyTorch libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, statistics\n",
    "import torchvision\n",
    "import torch\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce output size, we'll disable extra warnings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training small NN model\n",
    "We'll start from the simplest model: the neural network model and the simplest Convolutional model(LeNet).\\\n",
    "Both models will try to solve the image classification problem - try to recognize numbers from the image (MNIST dataset).\\\n",
    "First model will contrain three Linear layers with two activations(ReLU in our example):\\\n",
    " A Linear layer in PyTorch is a fundamental building block in neural networks that performs a linear transformation on the input data, often represented as a fully connected layer.\\\n",
    " The activation function, such as ReLU (Rectified Linear Unit), introduces non-linearity into the model, allowing it to learn more complex patterns. \\\n",
    " ReLU specifically sets all negative values to zero and leaves positive values unchanged, which helps to mitigate the vanishing gradient problem and improve training efficiency.\\\n",
    " \\\n",
    "Here you can see how you can create a class with model initialization and forward method, which will be used later in inference and training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork300(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 300),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300, 300),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional model additionally to Linear will use Convolution layer, which applies convolution operations to the input data. \\\n",
    "It uses a set of learnable filters (or kernels) that slide over the input to produce feature maps, capturing local patterns such as edges, textures, and shapes, making it particularly effective for image and signal processing tasks.\n",
    "![SegmentLocal](convolution.gif \"segment\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet_1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.AvgPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll define the train and evaluate functions, which expect the model, loss function and data for training and validation as arguments:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, loss_fn):\n",
    "    model.eval()\n",
    "    avg_vloss = 0.0\n",
    "    acc = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(data_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "            voutputs = model(vinputs)\n",
    "            vloss = loss_fn(voutputs, vlabels)\n",
    "            avg_vloss += vloss\n",
    "\n",
    "            _, indices = torch.max(voutputs, dim=1)\n",
    "            acc += torch.sum(indices == vlabels)\n",
    "    \n",
    "    avg_vloss = avg_vloss / (i + 1)\n",
    "    acc = acc / data_loader.dataset.data.shape[0]\n",
    "    return acc, avg_vloss\n",
    "\n",
    "\n",
    "def train(model, optimizer, loss_fn, training_loader, validation_loader, epochs):\n",
    "    train_time = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train(True)\n",
    "        running_loss = 0.\n",
    "        last_loss = 0.\n",
    "        start = time.time()\n",
    "        for i, data in enumerate(training_loader):\n",
    "            inputs, labels = data\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            outputs = model(inputs)\n",
    "        \n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "        \n",
    "            optimizer.step()\n",
    "            end = time.time()\n",
    "            train_time += end - start\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            if i % 1000 == 999:\n",
    "                last_loss = running_loss / 1000\n",
    "                print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "                running_loss = 0.\n",
    "            start = time.time()\n",
    "        \n",
    "        vpass, vloss = evaluate(model, validation_loader, loss_fn)\n",
    "        print(f'Epoch {epoch} train {last_loss} valid {vloss}')\n",
    "    print(f\"Training time: {train_time}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step will be data loading, we'll use dataset MNIST, which is provided by torchvision package:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = torchvision.datasets.MNIST(root='data', train=True, download=True, transform=ToTensor())\n",
    "validation_set = torchvision.datasets.MNIST(root='data', train=False, download=True, transform=ToTensor())\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=16, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=16, shuffle=False)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run training for basic neural model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = NeuralNetwork300()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "num_epochs = 5\n",
    "train(model, optimizer, loss_fn, training_loader, validation_loader, num_epochs)\n",
    "\n",
    "start = time.time()\n",
    "acc, loss = evaluate(model, validation_loader, loss_fn)\n",
    "end = time.time()\n",
    "print(f\"Inference time: {end - start}, Validation Passrate: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for the convolutional one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet_1()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "train(model, optimizer, loss_fn, training_loader, validation_loader, num_epochs)\n",
    "\n",
    "start = time.time()\n",
    "acc, loss = evaluate(model, validation_loader, loss_fn)\n",
    "end = time.time()\n",
    "print(f\"Inference time: {end - start}, Validation Passrate: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second(Convolutional) model works slower, but it solves the image classification problem better.\n",
    "\n",
    "Now lets define function for inference benchmarking. We'll use it later to compare different software optimizations and solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_benchmark(model, data_loader, num_iter, warm_up, ov=False):\n",
    "    if not ov:\n",
    "        model.eval()\n",
    "        current_device = model.conv1.weight.device\n",
    "    with torch.no_grad():\n",
    "        av_time = 0.0\n",
    "        for it in range(num_iter):\n",
    "            acc = 0.0\n",
    "            ev_time = 0.0\n",
    "            t0 = time.time()\n",
    "            for i, vdata in enumerate(data_loader):\n",
    "                vinputs, vlabels = vdata\n",
    "                if not ov:\n",
    "                    vinputs = vinputs.to(current_device)\n",
    "                    vlabels = vlabels.to(current_device)\n",
    "                voutputs = model(vinputs)\n",
    "                ev_time += time.time() - t0\n",
    "                if ov:\n",
    "                    voutputs = torch.Tensor(voutputs[0])\n",
    "                _, indices = torch.max(voutputs, dim=1)\n",
    "                acc += torch.sum(indices == vlabels)\n",
    "                t0 = time.time()\n",
    "            if it >= warm_up:\n",
    "                acc = acc / data_loader.dataset.data.shape[0]\n",
    "                av_time += ev_time\n",
    "                print(f\"{it - warm_up} iteration: {ev_time} pass_rate: {acc}\")\n",
    "        av_time = av_time / (num_iter - warm_up)\n",
    "        print(f\"Average inference time: {av_time}\")\n",
    "        return av_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our bencmarking we'll compare PyTorch model with [Intel OpenVINO](https://docs.openvino.ai/2024/home.html) optimizations. To do it we need to convert model to OpenVINO format.\\\n",
    "Additionaly OpenVINO supports [nncf](https://docs.openvino.ai/2022.3/nncf_ptq_introduction.html) weigthts compression, which optimize model by reducing weights size and using additional HW optimizations such as AVX512 and AMX depending on device capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ov_model(model, compressed=False, device=\"CPU\"):\n",
    "    from nncf import compress_weights\n",
    "    import openvino as ov\n",
    "    ov_model = ov.convert_model(model)\n",
    "    core = ov.Core()\n",
    "    if compressed:\n",
    "        compressed_model = compress_weights(ov_model)\n",
    "    compiled_model = core.compile_model(compressed_model if compressed else ov_model, device_name=device)\n",
    "    return compiled_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can prepare multiple versions of our trained model. First one will be [Intel PyTorch Extension](https://github.com/intel/intel-extension-for-pytorch) library. The second will be OpenVINO CPU version.\n",
    "If you have Intel HW with AMX/AVX512 or GPU(Intel or NVidia), then please comments in code to add such versions in your experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import intel_extension_for_pytorch as ipex\n",
    "ipex_model = ipex.optimize(model, inplace=False)\n",
    "\n",
    "ov_model = create_ov_model(model)\n",
    "\n",
    "models = {\"default\": model, \"IPEX\": ipex_model, \"OV_CPU\": ov_model}\n",
    "\n",
    "# if you have configured GPU (like in https://docs.openvino.ai/2024/get-started/configurations/configurations-intel-gpu.html)\n",
    "# then you can uncomment this lines:\n",
    "# gpu_ov_model = create_ov_model(model, device=\"GPU\")\n",
    "# models[\"OV_GPU\"] = gpu_ov_model\n",
    "\n",
    "# if your device support int8 instructions, then you can try to use compression:\n",
    "# compressed_ov_model = create_ov_model(model, compressed=True)\n",
    "# models[\"OV_Compressed\"] = compressed_ov_model\n",
    "\n",
    "# And if you have configured Nvidia GPU\n",
    "# then you can uncomment this lines to add it to testing:\n",
    "# gpu_cuda_model = copy.deepcopy(model).to(\"cuda\")\n",
    "# models[\"CUDA\"] = gpu_cuda_model\n",
    "\n",
    "model_stats = {}\n",
    "for m in models.keys():\n",
    "    print(f\"Testing model:{m}\")\n",
    "    model_stats[m] = inference_benchmark(models[m], validation_loader, 9, 1, m.startswith(\"OV\"))\n",
    "\n",
    "print(model_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language model example\n",
    "\n",
    "Lets move to more complex models - language models. In such case additionaly to PyTorch we'll use ```transformers``` library.\n",
    "This library provides API to load pretrained language models.\n",
    "In such models before applying model, we need to tokenize our input text by using special tokenizer.\n",
    "In our example we'll use Bert model to get token(```[MASK]```) prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "\n",
    "masked_text = \"I should change my [MASK]!\"\n",
    "inputs = tokenizer(masked_text, return_tensors=\"pt\")\n",
    "\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "logits = model(**inputs).logits\n",
    "mask_token_logits = logits[0, mask_token_index, :]\n",
    "top_3_tokens = torch.topk(mask_token_logits, 3, dim=1).indices[0].tolist()\n",
    "for token in top_3_tokens:\n",
    "    print(masked_text.replace(tokenizer.mask_token, tokenizer.decode([token])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we'll move to the most popular type of language models right now - Large Language Models for text generation. We'll start from GPT2 model, which doesn't require to obtain any accesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
    "input_text = \"What should I change?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, pad_token_id=tokenizer.eos_token_id, max_length=20)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like for Convolutional model let create small benchmark to compare performance of generative models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_generative(model, tokenizer, input_text, output_size, num_iter, warm_up):\n",
    "    t = list()\n",
    "    for i in range(num_iter):\n",
    "        start = time.time()\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "        outputs = model.generate(**inputs, pad_token_id=tokenizer.eos_token_id, max_length=output_size)\n",
    "        end = time.time()\n",
    "        if i >= warm_up:\n",
    "            t.append(end - start)\n",
    "            print(f\"{i - warm_up}: {end - start}s : {tokenizer.decode(outputs[0])}\")\n",
    "    print(f\"Average time: {statistics.fmean(t)}\")\n",
    "    return statistics.fmean(t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll compare default approach with Intel SW optimization on the facebook's opt small model.\n",
    "Additionaly we'll use transformers optimimum API to load model into OpenVINO format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")\n",
    "\n",
    "import intel_extension_for_pytorch as ipex\n",
    "ipex_model = ipex.llm.optimize(model.eval(), dtype=torch.float32, inplace=False, deployment_mode=True)\n",
    "\n",
    "from optimum.intel.openvino import OVModelForCausalLM\n",
    "ov_model = OVModelForCausalLM.from_pretrained(\"facebook/opt-125m\", export=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare them by using benchmark. As previously mentioned: if you have Intel or Nvidia GPU, please check comments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\"default\": model, \"IPEX\": ipex_model, \"OV_CPU\": ov_model}\n",
    "model_stats = {}\n",
    "# if you have configured GPU (like in https://docs.openvino.ai/2024/get-started/configurations/configurations-intel-gpu.html)\n",
    "# then you can uncomment this lines:\n",
    "# gpu_ov_model = OVModelForCausalLM.from_pretrained(\"facebook/opt-125m\", export=True, device=\"GPU\")\n",
    "# models[\"OV_GPU\"] = gpu_ov_model\n",
    "\n",
    "# And if you have configured Nvidia GPU\n",
    "# then you can uncomment this lines to add it to testing:\n",
    "# gpu_cuda_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\", device_map = 'cuda')\n",
    "# models[\"CUDA\"] = gpu_cuda_model\n",
    "for m in models.keys():\n",
    "    print(f\"Testing model:{m}\")\n",
    "    model_stats[m] = inference_generative(models[m], tokenizer, \"How are you?\", 20, 5, 1)\n",
    "\n",
    "print(model_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional materials: https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
